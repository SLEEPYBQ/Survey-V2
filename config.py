import argparse
import os
import torch

def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='PDF to Markdown è½¬æ¢åŠæ‰¹é‡é—®ç­”å·¥å…·')
    
    # åŸºæœ¬å‚æ•°
    parser.add_argument('--input-folder', '-i', default='pdfs', 
                       help='è¾“å…¥PDFæ–‡ä»¶å¤¹è·¯å¾„ (é»˜è®¤: pdfs)')
    parser.add_argument('--markdown-folder', '-m', default='markdowns', 
                       help='Markdownæ–‡ä»¶å¤¹è·¯å¾„ (é»˜è®¤: markdowns)')
    parser.add_argument('--output-folder', '-o', default='results', 
                       help='è¾“å‡ºç»“æœæ–‡ä»¶å¤¹è·¯å¾„ (é»˜è®¤: results)')
    
    # æ¨¡å¼é€‰æ‹©
    parser.add_argument('--mode', default='all', choices=['markdown', 'query', 'all'],
                       help='æ‰§è¡Œæ¨¡å¼: markdown(åªç”Ÿæˆmarkdown), query(åªæŸ¥è¯¢), all(ç”ŸæˆmarkdownåæŸ¥è¯¢) (é»˜è®¤: all)')
    
    # è®¾å¤‡å‚æ•° (ç”¨äºPDFè½¬æ¢)
    parser.add_argument('--device', default='auto', choices=['auto', 'cpu', 'cuda', 'mps'],
                       help='æŒ‡å®šè®¾å¤‡ (é»˜è®¤: autoè‡ªåŠ¨æ£€æµ‹)')
    parser.add_argument('--no-gpu', action='store_true',
                       help='å¼ºåˆ¶ä½¿ç”¨CPUï¼Œä¸ä½¿ç”¨ä»»ä½•GPU')
    
    # è½¬æ¢å‚æ•°
    parser.add_argument('--format-lines', action='store_true',
                       help='æ ¼å¼åŒ–æ–‡æœ¬è¡Œï¼Œæé«˜æ•°å­¦å…¬å¼è´¨é‡')
    parser.add_argument('--force-ocr', action='store_true',
                       help='å¼ºåˆ¶OCRå¤„ç†æ•´ä¸ªæ–‡æ¡£')
    
    # OpenAI APIå‚æ•°
    parser.add_argument('--api-key', default='xxx',
                       help='OpenAI APIå¯†é’¥')
    parser.add_argument('--api-base', default='https://api.openai-proxy.org/v1',
                       help='OpenAI API Base URL')
    parser.add_argument('--model', default='deepseek-chat',
                       help='ä½¿ç”¨çš„æ¨¡å‹')
    
    # å¹¶å‘å‚æ•°
    parser.add_argument('--max-workers', type=int, default=8,
                       help='æŸ¥è¯¢æ—¶çš„æœ€å¤§å¹¶å‘æ•° (é»˜è®¤: 4)')
    
    # å…¶ä»–å‚æ•°
    parser.add_argument('--verbose', '-v', action='store_true',
                       help='æ˜¾ç¤ºè¯¦ç»†è¾“å‡º')
    parser.add_argument('--dry-run', action='store_true',
                       help='è¯•è¿è¡Œï¼Œåªæ˜¾ç¤ºè¦å¤„ç†çš„æ–‡ä»¶ä½†ä¸å®é™…è½¬æ¢')
    
    return parser.parse_args()

def detect_device(args):
    """æ£€æµ‹å’Œè®¾ç½®è®¾å¤‡"""
    if args.no_gpu or args.device == 'cpu':
        device = 'cpu'
        print("ğŸ’» ä½¿ç”¨CPUæ¨¡å¼")
    elif args.device == 'cuda':
        if torch.cuda.is_available():
            device = 'cuda'
            print("ğŸš€ ä½¿ç”¨CUDA GPUæ¨¡å¼")
        else:
            device = 'cpu'
            print("âš ï¸  CUDAä¸å¯ç”¨ï¼Œå›é€€åˆ°CPUæ¨¡å¼")
    elif args.device == 'mps':
        if torch.backends.mps.is_available():
            device = 'mps'
            print("ğŸ ä½¿ç”¨MPSï¼ˆè‹¹æœGPUï¼‰æ¨¡å¼")
        else:
            device = 'cpu'
            print("âš ï¸  MPSä¸å¯ç”¨ï¼Œå›é€€åˆ°CPUæ¨¡å¼")
    else:  # auto
        if torch.cuda.is_available():
            device = 'cuda'
            print("ğŸš€ è‡ªåŠ¨æ£€æµ‹åˆ°CUDA GPUï¼Œä½¿ç”¨CUDAæ¨¡å¼")
        elif torch.backends.mps.is_available():
            device = 'mps'
            print("ğŸ è‡ªåŠ¨æ£€æµ‹åˆ°MPSï¼Œä½¿ç”¨è‹¹æœGPUæ¨¡å¼")
        else:
            device = 'cpu'
            print("ğŸ’» ä½¿ç”¨CPUæ¨¡å¼")
    
    # è®¾ç½®ç¯å¢ƒå˜é‡
    os.environ['TORCH_DEVICE'] = device
    return device

# é—®é¢˜IDåˆ—è¡¨ - ç”¨äºCSVåˆ—åï¼ˆä½¿ç”¨ä¸‹åˆ’çº¿æ ¼å¼ï¼Œä¸æç¤ºè¯ä¸­çš„æ ¼å¼ä¸€è‡´ï¼‰
QUESTION_IDS = [
    "involved_stakeholder", "sample_size", "country", "age", "gender",
    "demographic_background", "cognitive_and_physical_impairment", "needs_and_expectations",
    "application_context", "process_of_the_care", "methodology", "care_type",
    "robot_type", "robot_name", "design_goal", "robot_concern_function",
    "facilitating_functions", "inhibitory_functions", "stakeholder_facilitating_characteristics",
    "stakeholder_inhibitory_characteristics", "engagement", "acceptance", "trust",
    "key_findings", "additional_info", "testing_context"
]

# é—®é¢˜æ˜¾ç¤ºåç§°å’Œå¯¹åº”IDçš„æ˜ å°„ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
QUESTION_PATTERNS = [
    ("Involved Stakeholder", "involved_stakeholder"),
    ("Sample Size", "sample_size"),
    ("Country", "country"),
    ("Age", "age"),
    ("Gender", "gender"),
    ("Demographic Background", "demographic_background"),
    ("Cognitive And Physical Impairment", "cognitive_and_physical_impairment"),
    ("Needs And Expectations", "needs_and_expectations"),
    ("Application Context", "application_context"),
    ("Process Of The Care", "process_of_the_care"),
    ("Methodology", "methodology"),
    ("Care Type", "care_type"),
    ("Robot Type", "robot_type"),
    ("Robot Name", "robot_name"),
    ("Design Goal", "design_goal"),
    ("Robot Concern Function", "robot_concern_function"),
    ("Facilitating Functions", "facilitating_functions"),  
    ("Inhibitory Functions", "inhibitory_functions"),
    ("Stakeholder Facilitating Characteristics", "stakeholder_facilitating_characteristics"),
    ("Stakeholder Inhibitory Characteristics", "stakeholder_inhibitory_characteristics"),
    ("Engagement", "engagement"),
    ("Acceptance", "acceptance"),
    ("Trust", "trust"),
    ("Key Findings", "key_findings"),
    ("Additional Info", "additional_info"),
    ("Testing Context", "testing_context")
]