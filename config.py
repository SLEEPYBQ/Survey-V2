import argparse
import os
import torch

def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='PDF to Markdown è½¬æ¢åŠæ‰¹é‡é—®ç­”å·¥å…·')
    
    # åŸºæœ¬å‚æ•°
    parser.add_argument('--input-folder', '-i', default='pdfs', 
                       help='è¾“å…¥PDFæ–‡ä»¶å¤¹è·¯å¾„ (é»˜è®¤: pdfs)')
    parser.add_argument('--markdown-folder', '-m', default='markdowns', 
                       help='Markdownæ–‡ä»¶å¤¹è·¯å¾„ (é»˜è®¤: markdowns)')
    parser.add_argument('--output-folder', '-o', default='results', 
                       help='è¾“å‡ºç»“æœæ–‡ä»¶å¤¹è·¯å¾„ (é»˜è®¤: results)')
    
    # æ¨¡å¼é€‰æ‹©
    parser.add_argument('--mode', default='all', choices=['markdown', 'query', 'all'],
                       help='æ‰§è¡Œæ¨¡å¼: markdown(åªç”Ÿæˆmarkdown), query(åªæŸ¥è¯¢), all(ç”ŸæˆmarkdownåæŸ¥è¯¢) (é»˜è®¤: all)')
    
    # è®¾å¤‡å‚æ•° (ç”¨äºPDFè½¬æ¢)
    parser.add_argument('--device', default='cuda', choices=['auto', 'cpu', 'cuda', 'mps'],
                       help='æŒ‡å®šè®¾å¤‡ (é»˜è®¤: autoè‡ªåŠ¨æ£€æµ‹)')
    parser.add_argument('--no-gpu', action='store_true',
                       help='å¼ºåˆ¶ä½¿ç”¨CPUï¼Œä¸ä½¿ç”¨ä»»ä½•GPU')
    
    # è½¬æ¢å‚æ•°
    parser.add_argument('--format-lines', action='store_true',
                       help='æ ¼å¼åŒ–æ–‡æœ¬è¡Œï¼Œæé«˜æ•°å­¦å…¬å¼è´¨é‡')
    parser.add_argument('--force-ocr', action='store_true',
                       help='å¼ºåˆ¶OCRå¤„ç†æ•´ä¸ªæ–‡æ¡£')
    
    # OpenAI APIå‚æ•°
    parser.add_argument('--api-key', default='sk-wqEoMUckbUlrTL4tVcz6kOvn1Gw9Uj499VahouSP83cdtv4c',
                       help='OpenAI APIå¯†é’¥')
    parser.add_argument('--api-base', default='https://api.openai-proxy.org/v1',
                       help='OpenAI API Base URL')
    parser.add_argument('--model', default='deepseek-chat',
                       help='ä½¿ç”¨çš„æ¨¡å‹')
    
    # å¹¶å‘å‚æ•°
    parser.add_argument('--max-workers', type=int, default=8,
                       help='æŸ¥è¯¢æ—¶çš„æœ€å¤§å¹¶å‘æ•° (é»˜è®¤: 4)')
    
    # å…¶ä»–å‚æ•°
    parser.add_argument('--verbose', '-v', action='store_true',
                       help='æ˜¾ç¤ºè¯¦ç»†è¾“å‡º')
    parser.add_argument('--dry-run', action='store_true',
                       help='è¯•è¿è¡Œï¼Œåªæ˜¾ç¤ºè¦å¤„ç†çš„æ–‡ä»¶ä½†ä¸å®é™…è½¬æ¢')
    
    return parser.parse_args()

def detect_device(args):
    """æ£€æµ‹å’Œè®¾ç½®è®¾å¤‡"""
    if args.no_gpu or args.device == 'cpu':
        device = 'cpu'
        print("ğŸ’» ä½¿ç”¨CPUæ¨¡å¼")
    elif args.device == 'cuda':
        if torch.cuda.is_available():
            device = 'cuda'
            print("ğŸš€ ä½¿ç”¨CUDA GPUæ¨¡å¼")
        else:
            device = 'cpu'
            print("âš ï¸  CUDAä¸å¯ç”¨ï¼Œå›é€€åˆ°CPUæ¨¡å¼")
    elif args.device == 'mps':
        if torch.backends.mps.is_available():
            device = 'mps'
            print("ğŸ ä½¿ç”¨MPSï¼ˆè‹¹æœGPUï¼‰æ¨¡å¼")
        else:
            device = 'cpu'
            print("âš ï¸  MPSä¸å¯ç”¨ï¼Œå›é€€åˆ°CPUæ¨¡å¼")
    else:  # auto
        if torch.cuda.is_available():
            device = 'cuda'
            print("ğŸš€ è‡ªåŠ¨æ£€æµ‹åˆ°CUDA GPUï¼Œä½¿ç”¨CUDAæ¨¡å¼")
        elif torch.backends.mps.is_available():
            device = 'mps'
            print("ğŸ è‡ªåŠ¨æ£€æµ‹åˆ°MPSï¼Œä½¿ç”¨è‹¹æœGPUæ¨¡å¼")
        else:
            device = 'cpu'
            print("ğŸ’» ä½¿ç”¨CPUæ¨¡å¼")
    
    # è®¾ç½®ç¯å¢ƒå˜é‡
    os.environ['TORCH_DEVICE'] = device
    return device

# é—®é¢˜IDåˆ—è¡¨ - ç”¨äºXLSXåˆ—å
QUESTION_IDS = [
    "involved_stakeholder",
    "sample_size",
    "country",
    "age",
    "gender",
    "demographic_background",
    "technology_literacy",
    "cognitive_and_physical_impairment",
    "needs_and_expectations",
    "application_context",
    "testing_context",
    "process_of_the_care",
    "methodology",
    "robot_type",
    "robot_name",
    "design_goal",
    "robot_general_function",
    "facilitating_functions",
    "inhibitory_functions",
    "stakeholder_facilitating_characteristics",
    "stakeholder_inhibitory_characteristics",
    "engagement",
    "acceptance",
    "robot_function_effectiveness",
    "user_satisfaction",
    "user_curiosity",
    "user_trust_reliance",
    "user_understanding",
    "learning_curve_productivity",
    "system_controllability_interaction",
    "key_findings",
    "additional_info"
]

# é—®é¢˜æ˜¾ç¤ºåç§°å’Œå¯¹åº”IDçš„æ˜ å°„
QUESTION_PATTERNS = [
    ("Involved Stakeholder", "involved_stakeholder"),
    ("Sample Size", "sample_size"),
    ("Country", "country"),
    ("Age", "age"),
    ("Gender", "gender"),
    ("Demographic Background", "demographic_background"),
    ("Technology Literacy", "technology_literacy"),
    ("Cognitive and Physical Impairment", "cognitive_and_physical_impairment"),
    ("Needs and Expectations", "needs_and_expectations"),
    ("Application Context", "application_context"),
    ("Testing Context", "testing_context"),
    ("Process of the Care", "process_of_the_care"),
    ("Methodology", "methodology"),
    ("Robot Type", "robot_type"),
    ("Robot Name", "robot_name"),
    ("Design Goal", "design_goal"),
    ("Robot General Function", "robot_general_function"),
    ("Facilitating Functions", "facilitating_functions"),
    ("Inhibitory Functions", "inhibitory_functions"),
    ("Stakeholder Facilitating Characteristics", "stakeholder_facilitating_characteristics"),
    ("Stakeholder Inhibitory Characteristics", "stakeholder_inhibitory_characteristics"),
    ("Engagement", "engagement"),
    ("Acceptance", "acceptance"),
    ("Robot Function Effectiveness", "robot_function_effectiveness"),
    ("User Satisfaction", "user_satisfaction"),
    ("User Curiosity", "user_curiosity"),
    ("User Trust Reliance", "user_trust_reliance"),
    ("User Understanding", "user_understanding"),
    ("Learning Curve Productivity", "learning_curve_productivity"),
    ("System Controllability Interaction", "system_controllability_interaction"),
    ("Key Findings", "key_findings"),
    ("Additional Info", "additional_info")
]