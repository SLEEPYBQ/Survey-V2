import os
import glob
import re
import json
from datetime import datetime
from concurrent.futures import ProcessPoolExecutor
from openai import OpenAI
from tqdm import tqdm
from config import QUESTION_PATTERNS, QUESTION_IDS

def create_combined_prompt(markdown_content):
    """åˆ›å»ºåˆå¹¶çš„æç¤ºè¯"""
    
    prompt_template = """Please analyze the provided research paper and answer the following questions.

CRITICAL FORMAT REQUIREMENTS:
1. You MUST use the exact format shown below for EVERY question
2. Each question MUST start with ## followed by the exact question ID
3. The answer MUST start with "Answer: " on a new line
4. The source MUST start with "Source: " on a new line
5. DO NOT add any extra text, formatting, or explanations outside this format
6. If information is not available, write "Answer: N/A" and "Source: Not found in the paper"
7. The Source MUST include the specific sources of all key information, including figure or table numbers (e.g., "Figure 1", "Table 2") if applicable
8. All content of the Source Must be directly quoted or derived from the original text of the paper.

EXACT FORMAT TEMPLATE (you must follow this precisely):
## question_id
Answer: [Your concise answer here]
Source: [Quote the relevant text from the paper]

QUESTIONS:
1. involved_stakeholder: What are the involved stakeholders in the study? Stakeholders include primary subjects or interviewees. Use the title of the role explicitly mentioned in the text (e.g., care staff, elderly individuals).

2. sample_size: What is the sample size of the study? For example, if 100 people participated and only 90 consented to data collection, the sample size is 90. For multi-study papers, specify the sample size for each study group.

3. country: What is the country or region of the participants as explicitly stated in the paper (do not infer from the authors' affiliations)?

4. culture: Whether the study had any adaptations or accommodations or examination of cultural considerations? If yes, please elaborate.Â  

5. age: What age-related numerical information is provided in the study (e.g., age range like 65-80, mean age like 72, or median age like 70)? Exclude vague descriptions such as "older adults" or "elderly".

6. gender: What gender-related information is reported in the study?

7. demographic_background: What demographic background information is reported? For example, socioeconomic status, educational level, living context for elderly people, or working context for caregivers. Include additional details such as language proficiency or professional background if mentioned. Exclude technology literacy. Use the exact terminology from the original text. Provide as comprehensive a response as possible, including all relevant information.

8. technology_literacy: What information is provided about the technology literacy of stakeholders? This includes their experience interacting with robots, understanding of robots, acceptance of technology, or frequency of using technological products.

9. cognitive_and_physical_impairment: What cognitive and physical impairments are described among the elderly participants? Separate cognitive impairments (e.g., memory issues, dementia) and physical impairments (e.g., mobility issues, arthritis) in the output. If standardized measurement tools were used, report the specific scores and scores of the scale; if qualitative terms (e.g., 'mild', 'severe') were used, report them accordingly. If no measurement tools or qualitative descriptions are used, do not extract information.

10. needs_and_expectations: What are the explicitly stated or inferred needs and expectations of users, primarily elderly people and caregivers? This includes directly expressed needs and user preferences accompanied by explanatory comments during interviews or post-trial reflections. In the output, clearly separate needs (e.g., functional requirements) and expectations (e.g., desired outcomes or preferences). Use the exact terminology from the original text. Provide as comprehensive a response as possible, including all relevant information.

11. application_context: What is the envisioned future application context for the robot as mentioned in the paper? Check the introduction, discussion or conclusion sections that may describe potential future application scenarios, not the current testing scenarios.

12. testing_context: What is the testing context of the study? (For example, was the test conducted in a lab, care home, hospital, private residence, or another setting?)

13. process_of_the_care: What information is provided about the duration and stage of the care process? Specify whether the study involved a first encounter, short-term use, or long-term deployment, and include session duration and frequency if available. If multiple experiments are involved, describe the care process for each experiment.

14. methodology: What research methodology was used in the study (e.g., qualitative interviews, quantitative surveys, randomized controlled trials)?

15. robot_type: What type of robot is used in the study? (If the paper uses terms like 'human-like' or 'animal-like', use those directly; otherwise, provide a short description of the robot's appearance.)

16. robot_name: What is the name of the robot used in the study?

17. design_goal: What design goals were set for the robot or its interaction functions in the current study? These are typically found in the aim, objective, experimental design, or discussion sections of the paper.

18. robot_general_function: What functionalities of the robot were demonstrated, deployed, or introduced to users during the study?

19. facilitating_functions: What specific robot functions or features are reported to enhance the user experience (i.e., positive features)? Provide a comprehensive list of all relevant information. If multiple robots are used, list results associated with each robot, explicitly referencing the robot's name. Include brief explanations for why these features are considered beneficial. Provide as comprehensive a response as possible, including all relevant information.

20. inhibitory_functions: What specific robot functions or features are reported to hinder the user experience (i.e., negative features)? Provide a comprehensive list of all relevant information. If multiple robots are used, list results associated with each robot, explicitly referencing the robot's name. Include brief explanations for why these features are considered detrimental. Provide as comprehensive a response as possible, including all relevant information.

21. stakeholder_facilitating_characteristics: What inherent characteristics of the stakeholders are associated with better robot use, acceptance, or trust? Provide a comprehensive list of all relevant information, focusing on user characteristics (e.g., personality traits, prior experience) rather than changes or behaviors post-robot interaction. If multiple robots are used, list results associated with each robot, explicitly referencing the robot's name. Include brief explanations where available. Provide as comprehensive a response as possible, including all relevant information.

22. stakeholder_inhibitory_characteristics: What inherent characteristics of the stakeholders are associated with reduced robot use, lower acceptance, or lower trust? Provide a comprehensive list of all relevant information, focusing on user characteristics (e.g., personality traits, prior experience) rather than changes or behaviors post-robot interaction. If multiple robots are used, list results associated with each robot, explicitly referencing the robot's name. Include brief explanations where available. Provide as comprehensive a response as possible, including all relevant information.

23. engagement: What evaluation of user engagement in the robot is reported in the study? This may include quantitative measurements (e.g., rating scales) or qualitative descriptions (e.g., 'high engagement', 'low acceptance', 'gradual trust development').

24. acceptance: What evaluation of user acceptance in the robot is reported in the study? This may include quantitative measurements (e.g., rating scales) or qualitative descriptions (e.g., 'high engagement', 'low acceptance', 'gradual trust development').

25. robot_function_effectiveness: What evaluations of the performance and effectiveness of the robot's functions are reported in the study? What are the results of these evaluations? This may include quantitative metrics (e.g., accuracy) or qualitative assessments (e.g., 'high effectiveness', 'low effectiveness').

26. user_satisfaction: What evaluations of user satisfaction with the robot are reported in the study? What are the results of these evaluations? User satisfaction refers to the subjective feelings of fulfillment a user experiences regarding the robot. This may include quantitative metrics (e.g., rating scales) or qualitative descriptions (e.g., 'high satisfaction', 'low satisfaction').

27. user_curiosity: What evaluations of the user's curiosity about robots are reported in the study? What are the results of these evaluations? This may include quantitative metrics (e.g., rating scales) or qualitative descriptions (e.g., 'high curiosity', 'low curiosity').

28. user_trust_reliance: What evaluations of the user's trust in robots are reported in the study? What are the results of these evaluations? This may include quantitative metrics (e.g., rating scales) or qualitative descriptions (e.g., 'high trust', 'low trust').

29. user_understanding: What evaluations of the user's understanding of robots are reported in the study? What are the results of these evaluations? This may include quantitative metrics (e.g., rating scales) or qualitative descriptions (e.g., 'high level of understanding', 'low level of understanding').

30. learning_curve_productivity: What evaluations of

31. system_controllability_interaction: What evaluations of the robotsâ€™ controllability and adaptiveness are reported in the study? What are the results of these evaluations? Controllability and interaction refer to whether the robot is context-aware, flexible, and tailored to meet the specific needs and preferences of users. This may include quantitative metrics (e.g., rating scales) or qualitative descriptions (e.g., 'high controllability and adaptiveness', 'low adaptiveness').

32. key_findings: What are the key findings of the study, as typically summarized in the conclusion or discussion section?

33. additional_info: What additional information is provided about the study, such as limitations or other relevant details?

RESPONSE FORMAT EXAMPLE:
## involved stakeholder
Answer: Elderly individuals, care staff
Source: The study included ten elderly individuals and five care staff as interviewees

## robot function effectiveness
Answer: The robot achieved 85 percent accuracy in task completion
Source: The study reported an accuracy of 85percent for the robot's task performance

[Continue for all 32 questions using their exact question IDs]

IMPORTANT REMINDERS:
- Use the exact question IDs: involved_stakeholder, sample_size, country, culture, age, gender, demographic_background, technology_literacy, cognitive_and_physical_transformation, needs_and_requirements, application_context, testing_context, process_of_the_care, methodology, robot_type, robot_name, design_goal, robot_general_function, facilitating_functions, inhibitory_functions, stakeholder_facilitating_characteristics, stakeholder_inhibitory_characteristics, engagement, acceptance, robot_function_effectiveness, user_satisfaction, user_curiosity, user_trust_reliance, user_understanding, learning_curve_productivity, system_controllability_interaction, key_findings, additional_info
- Each answer MUST start with "Answer: "
- Each source MUST start with "Source: "
- Do not use any other formatting

RESEARCH PAPER CONTENT:
{content}"""
    
    return prompt_template.format(content=markdown_content)

def save_raw_response(doc_name, response_text, raw_response_dir='raw_responses'):
    """ä¿å­˜LLMçš„åŸå§‹å“åº”åˆ°æ–‡ä»¶"""
    # åˆ›å»ºåŸå§‹å“åº”ç›®å½•
    os.makedirs(raw_response_dir, exist_ok=True)
    
    # ç”Ÿæˆæ–‡ä»¶åï¼ˆç§»é™¤.mdæ‰©å±•åï¼Œæ·»åŠ æ—¶é—´æˆ³ï¼‰
    base_name = os.path.splitext(doc_name)[0]
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"{base_name}_{timestamp}.txt"
    filepath = os.path.join(raw_response_dir, filename)
    
    # ä¿å­˜åŸå§‹å“åº”
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(f"Document: {doc_name}\n")
        f.write(f"Timestamp: {timestamp}\n")
        f.write("="*80 + "\n\n")
        f.write(response_text)
    
    return filepath

def query_document_with_combined_questions(markdown_path, client, model, verbose=False):
    """ä½¿ç”¨åˆå¹¶é—®é¢˜æŸ¥è¯¢å•ä¸ªæ–‡æ¡£"""
    try:
        # è¯»å–Markdownæ–‡ä»¶
        with open(markdown_path, 'r', encoding='utf-8') as f:
            markdown_content = f.read()
        
        # åˆ›å»ºåˆå¹¶çš„æç¤ºè¯
        combined_prompt = create_combined_prompt(markdown_content)
        
        if verbose:
            print(f"    å‘é€æŸ¥è¯¢è¯·æ±‚...")
        
        # å‘é€è¯·æ±‚åˆ°OpenAI
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "user", "content": combined_prompt}
            ],
            temperature=0.0,
            max_tokens=10000
        )
        
        result_text = response.choices[0].message.content
        
        # ä¿å­˜åŸå§‹å“åº”
        doc_name = os.path.basename(markdown_path)
        raw_response_path = save_raw_response(doc_name, result_text)
        
        if verbose:
            print(f"    âœ… æŸ¥è¯¢æˆåŠŸ")
            print(f"    ğŸ’¾ åŸå§‹å“åº”å·²ä¿å­˜åˆ°: {raw_response_path}")
        
        return True, result_text
        
    except Exception as e:
        if verbose:
            print(f"    âŒ æŸ¥è¯¢å¤±è´¥: {e}")
        return False, str(e)

def parse_combined_response(response_text):
    """è§£æåˆå¹¶å“åº”ï¼Œæå–å„ä¸ªé—®é¢˜çš„ç­”æ¡ˆ - ä½¿ç”¨ç®€åŒ–çš„æ­£åˆ™è¡¨è¾¾å¼"""
    results = {}
    
    # ç®€åŒ–çš„æ­£åˆ™è¡¨è¾¾å¼ - åªåŒ¹é…ä¸¥æ ¼çš„æ ¼å¼
    # åŒ¹é…æ¨¡å¼: ## question_id\nAnswer: xxx\nSource: xxx
    pattern = r'##\s*(\w+)\s*\n\s*Answer:\s*(.*?)\n\s*Source:\s*(.*?)(?=\n\s*##|\Z)'
    matches = re.findall(pattern, response_text, re.DOTALL | re.MULTILINE)
    # pattern = r'##\s*([^\n#]+)\s*\n\s*Answer:\s*(.*?)\n\s*Source:\s*(.*?)(?=\n\s*##|\Z)'
    # matches = re.findall(pattern, response_text, re.DOTALL | re.MULTILINE)
    
    # å°†åŒ¹é…ç»“æœè½¬æ¢ä¸ºå­—å…¸
    for question_id, answer, source in matches:
        # æ¸…ç†ç­”æ¡ˆå’Œæºæ–‡æœ¬ä¸­çš„å¤šä½™ç©ºç™½
        answer = answer.strip()
        source = source.strip()
        
        # ç»„åˆç­”æ¡ˆå’Œæº
        results[question_id] = f"{answer}\nSource: {source}"
    
    # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰é—®é¢˜éƒ½è¢«è§£æ
    missing_questions = []
    for _, question_id in QUESTION_PATTERNS:
        if question_id not in results:
            missing_questions.append(question_id)
            results[question_id] = "[è§£æå¤±è´¥] - æ— æ³•ä»å“åº”ä¸­æå–ç­”æ¡ˆ"
    
    if missing_questions and len(missing_questions) < 26:  # å¦‚æœåªæ˜¯éƒ¨åˆ†å¤±è´¥
        # å°è¯•å¤‡ç”¨è§£ææ–¹æ³• - æ›´å®½æ¾çš„åŒ¹é…
        for question_id in missing_questions:
            # å°è¯•åŒ¹é…å˜ä½“æ ¼å¼
            alt_patterns = [
                # æ ‡å‡†æ ¼å¼ä½†å¯èƒ½æœ‰ç»†å¾®å·®å¼‚
                rf'##\s*{re.escape(question_id)}\s*\n(.*?)(?=\n\s*##|\Z)',
                # å¯èƒ½Answerå’ŒSourceåœ¨åŒä¸€æ®µè½
                rf'##\s*{re.escape(question_id)}\s*\n\s*Answer:\s*(.*?)\s*Source:\s*(.*?)(?=\n\s*##|\Z)',
            ]
            
            for alt_pattern in alt_patterns:
                match = re.search(alt_pattern, response_text, re.DOTALL | re.MULTILINE | re.IGNORECASE)
                if match:
                    if len(match.groups()) == 1:
                        # æ•´æ®µåŒ¹é…ï¼Œéœ€è¦åˆ†ç¦»Answerå’ŒSource
                        content = match.group(1).strip()
                        answer_match = re.search(r'Answer:\s*(.*?)(?=Source:|$)', content, re.DOTALL)
                        source_match = re.search(r'Source:\s*(.*?)$', content, re.DOTALL)
                        
                        if answer_match and source_match:
                            answer = answer_match.group(1).strip()
                            source = source_match.group(1).strip()
                            results[question_id] = f"{answer}\nSource: {source}"
                            break
                    else:
                        # å·²ç»åˆ†ç¦»çš„Answerå’ŒSource
                        answer = match.group(1).strip()
                        source = match.group(2).strip()
                        results[question_id] = f"{answer}\nSource: {source}"
                        break
    
    return results

def query_documents_wrapper(args_tuple):
    """å¹¶è¡ŒæŸ¥è¯¢æ–‡æ¡£çš„åŒ…è£…å‡½æ•°"""
    markdown_path, api_key, api_base, model, verbose = args_tuple
    
    try:
        # åˆ›å»ºOpenAIå®¢æˆ·ç«¯
        client = OpenAI(
            api_key=api_key,
            base_url=api_base
        )
        
        doc_name = os.path.basename(markdown_path)
        
        if verbose:
            print(f"  ğŸ”„ æŸ¥è¯¢æ–‡æ¡£: {doc_name}")
        
        # æŸ¥è¯¢æ–‡æ¡£
        success, response = query_document_with_combined_questions(
            markdown_path, client, model, verbose
        )
        
        if success:
            # è§£æå“åº”
            parsed_results = parse_combined_response(response)
            
            # éªŒè¯è§£æç»“æœ
            parse_failures = sum(1 for v in parsed_results.values() if "[è§£æå¤±è´¥]" in v)
            if parse_failures > 0:
                print(f"  âš ï¸  {doc_name}: {parse_failures}ä¸ªé—®é¢˜è§£æå¤±è´¥")
            
            if verbose:
                print(f"  âœ… å®Œæˆ: {doc_name}")
            
            return doc_name, True, parsed_results, None
        else:
            if verbose:
                print(f"  âŒ å¤±è´¥: {doc_name}")
            return doc_name, False, None, response
            
    except Exception as e:
        doc_name = os.path.basename(markdown_path) if markdown_path else "æœªçŸ¥æ–‡æ¡£"
        return doc_name, False, None, str(e)

def query_all_documents(args):
    """æŸ¥è¯¢æ‰€æœ‰Markdownæ–‡æ¡£"""
    
    # æŸ¥æ‰¾æ‰€æœ‰Markdownæ–‡ä»¶
    markdown_files = glob.glob(os.path.join(args.markdown_folder, "*.md"))
    
    if not markdown_files:
        print(f"âŒ åœ¨ {args.markdown_folder} æ–‡ä»¶å¤¹ä¸­æ²¡æœ‰æ‰¾åˆ°Markdownæ–‡ä»¶")
        return
    
    print(f"ğŸ“„ æ‰¾åˆ° {len(markdown_files)} ä¸ªMarkdownæ–‡ä»¶")
    
    # APIé…ç½®
    api_key = args.api_key or os.getenv('OPENAI_API_KEY')
    if not api_key:
        print("âŒ è¯·æä¾›OpenAI APIå¯†é’¥ (é€šè¿‡--api-keyå‚æ•°æˆ–OPENAI_API_KEYç¯å¢ƒå˜é‡)")
        return
    
    # åˆ›å»ºåŸå§‹å“åº”ç›®å½•
    os.makedirs('raw_responses', exist_ok=True)
    print(f"ğŸ“ åŸå§‹å“åº”å°†ä¿å­˜åˆ°: raw_responses/")
    
    # å‡†å¤‡æŸ¥è¯¢å‚æ•°
    query_args = [
        (md_path, api_key, args.api_base, args.model, args.verbose)
        for md_path in markdown_files
    ]
    
    print(f"ğŸš€ ä½¿ç”¨ {args.max_workers} ä¸ªå·¥ä½œè¿›ç¨‹å¹¶è¡ŒæŸ¥è¯¢...")
    print("-" * 50)
    
    # å­˜å‚¨æ‰€æœ‰ç»“æœ
    all_results = {}
    failed_queries = []
    parse_stats = {'total': 0, 'failures': 0}
    
    # å¹¶è¡Œæ‰§è¡ŒæŸ¥è¯¢
    with ProcessPoolExecutor(max_workers=args.max_workers) as executor:
        for doc_name, success, results, error in tqdm(
            executor.map(query_documents_wrapper, query_args),
            total=len(query_args),
            desc="æŸ¥è¯¢è¿›åº¦"
        ):
            if success:
                # ç»Ÿè®¡è§£æå¤±è´¥
                for v in results.values():
                    if "[è§£æå¤±è´¥]" in v:
                        parse_stats['failures'] += 1
                parse_stats['total'] += len(results)
                
                # æ„å»ºç»“æœå­—å…¸
                doc_result = {"document": doc_name}
                doc_result.update(results)
                all_results[doc_name] = doc_result
            else:
                print(f"âŒ {doc_name}: {error}")
                failed_queries.append((doc_name, error))
                # ä¸ºå¤±è´¥çš„æŸ¥è¯¢æ·»åŠ å ä½ç¬¦
                doc_result = {"document": doc_name}
                for question_id in QUESTION_IDS:
                    doc_result[question_id] = f"[æŸ¥è¯¢å¤±è´¥] - {error}"
                all_results[doc_name] = doc_result
    
    # è¾“å‡ºç»Ÿè®¡
    print("\n" + "=" * 50)
    print("ğŸ“Š æŸ¥è¯¢å®Œæˆç»Ÿè®¡:")
    print(f"âœ… æˆåŠŸæŸ¥è¯¢: {len(all_results) - len(failed_queries)} ä¸ªæ–‡æ¡£")
    print(f"âŒ æŸ¥è¯¢å¤±è´¥: {len(failed_queries)} ä¸ªæ–‡æ¡£")
    
    if parse_stats['total'] > 0:
        success_rate = (parse_stats['total'] - parse_stats['failures']) / parse_stats['total'] * 100
        print(f"ğŸ“ˆ è§£ææˆåŠŸç‡: {success_rate:.1f}% ({parse_stats['total'] - parse_stats['failures']}/{parse_stats['total']} ä¸ªé—®é¢˜)")
    
    if failed_queries:
        print(f"\nâŒ å¤±è´¥çš„æŸ¥è¯¢è¯¦æƒ…:")
        for doc_name, error in failed_queries[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªé”™è¯¯
            print(f"  â€¢ {doc_name}: {error}")
        if len(failed_queries) > 5:
            print(f"  ... è¿˜æœ‰ {len(failed_queries) - 5} ä¸ªå¤±è´¥çš„æŸ¥è¯¢")
    
    # è°ƒè¯•ï¼šå¦‚æœæœ‰è§£æå¤±è´¥ï¼Œæ˜¾ç¤ºä¸€äº›ç»†èŠ‚
    if parse_stats['failures'] > 0:
        print(f"\nâš ï¸  è§£æå¤±è´¥è°ƒè¯•ä¿¡æ¯:")
        # æ‰¾å‡ºå“ªäº›é—®é¢˜æœ€å®¹æ˜“å¤±è´¥
        question_failure_count = {}
        for doc_result in all_results.values():
            for question_id in QUESTION_IDS:
                if question_id in doc_result and "[è§£æå¤±è´¥]" in doc_result[question_id]:
                    question_failure_count[question_id] = question_failure_count.get(question_id, 0) + 1
        
        if question_failure_count:
            sorted_failures = sorted(question_failure_count.items(), key=lambda x: x[1], reverse=True)
            print("  æœ€å¸¸å¤±è´¥çš„é—®é¢˜:")
            for question_id, count in sorted_failures[:5]:
                print(f"    â€¢ {question_id}: {count}æ¬¡å¤±è´¥")
    
    return all_results
