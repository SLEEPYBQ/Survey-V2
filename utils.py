import os
import csv
import json
import glob
from datetime import datetime
from config import QUESTION_IDS

def save_results_to_csv(all_results, output_dir):
    """ä¿å­˜ç»“æœåˆ°CSVæ–‡ä»¶"""
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    # æ„å»ºCSVçš„å­—æ®µåï¼ˆè¡¨å¤´ï¼‰- ä¸é™æ€æç¤ºè¯ä¸­çš„é—®é¢˜é¡ºåºä¸€è‡´
    fieldnames = ['document'] + QUESTION_IDS
    
    # ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„æ–‡ä»¶å
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    csv_filename = f"query_results_{timestamp}.csv"
    csv_path = os.path.join(output_dir, csv_filename)
    
    # åŒæ—¶åˆ›å»ºä¸€ä¸ªæœ€æ–°ç»“æœçš„é“¾æ¥ï¼ˆè¦†ç›–ï¼‰
    latest_csv_path = os.path.join(output_dir, "query_results_latest.csv")
    
    # ç»Ÿè®¡ä¿¡æ¯
    stats = {
        'total_documents': len(all_results),
        'parse_failures': 0,
        'query_failures': 0,
        'empty_answers': 0
    }
    
    # å°†ç»“æœä¿å­˜ä¸ºCSV
    with open(csv_path, 'w', newline='', encoding='utf-8') as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()
        
        # å†™å…¥æ¯ä¸ªæ–‡æ¡£çš„æ‰€æœ‰é—®é¢˜ç»“æœ
        for doc_name, results in all_results.items():
            # ç»Ÿè®¡è§£æå¤±è´¥å’ŒæŸ¥è¯¢å¤±è´¥
            for question_id in QUESTION_IDS:
                if question_id in results:
                    if "[è§£æå¤±è´¥]" in results[question_id]:
                        stats['parse_failures'] += 1
                    elif "[æŸ¥è¯¢å¤±è´¥]" in results[question_id]:
                        stats['query_failures'] += 1
                    elif results[question_id].strip() == "N/A" or results[question_id].strip() == "":
                        stats['empty_answers'] += 1
            
            writer.writerow(results)
    
    # å¤åˆ¶åˆ°æœ€æ–°ç»“æœæ–‡ä»¶
    with open(csv_path, 'r', encoding='utf-8') as src:
        content = src.read()
    with open(latest_csv_path, 'w', encoding='utf-8') as dst:
        dst.write(content)
    
    # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
    stats_path = os.path.join(output_dir, f"query_stats_{timestamp}.json")
    with open(stats_path, 'w', encoding='utf-8') as f:
        json.dump(stats, f, indent=2)
    
    print(f"\nğŸ’¾ æŸ¥è¯¢ç»“æœå·²ä¿å­˜åˆ°:")
    print(f"   - ä¸»æ–‡ä»¶: {csv_path}")
    print(f"   - æœ€æ–°ç»“æœ: {latest_csv_path}")
    print(f"   - ç»Ÿè®¡ä¿¡æ¯: {stats_path}")
    
    # æ˜¾ç¤ºç»Ÿè®¡æ‘˜è¦
    if stats['total_documents'] > 0:
        total_questions = stats['total_documents'] * len(QUESTION_IDS)
        successful_parses = total_questions - stats['parse_failures'] - stats['query_failures']
        print(f"\nğŸ“Š æ•°æ®è´¨é‡ç»Ÿè®¡:")
        print(f"   - æˆåŠŸè§£æ: {successful_parses}/{total_questions} ({successful_parses/total_questions*100:.1f}%)")
        print(f"   - ç©ºå€¼/N/A: {stats['empty_answers']} ({stats['empty_answers']/total_questions*100:.1f}%)")
        print(f"   - è§£æå¤±è´¥: {stats['parse_failures']} ({stats['parse_failures']/total_questions*100:.1f}%)")
    
    return csv_path

def validate_raw_responses(raw_response_dir='raw_responses'):
    """éªŒè¯åŸå§‹å“åº”ç›®å½•ä¸­çš„æ–‡ä»¶"""
    if not os.path.exists(raw_response_dir):
        print(f"âš ï¸  åŸå§‹å“åº”ç›®å½•ä¸å­˜åœ¨: {raw_response_dir}")
        return
    
    response_files = glob.glob(os.path.join(raw_response_dir, "*.txt"))
    
    if not response_files:
        print(f"âš ï¸  åŸå§‹å“åº”ç›®å½•ä¸ºç©º: {raw_response_dir}")
        return
    
    print(f"\nğŸ“ åŸå§‹å“åº”éªŒè¯:")
    print(f"   - æ‰¾åˆ° {len(response_files)} ä¸ªå“åº”æ–‡ä»¶")
    
    # æ£€æŸ¥æ–‡ä»¶å¤§å°
    total_size = 0
    min_size = float('inf')
    max_size = 0
    
    for file_path in response_files:
        size = os.path.getsize(file_path)
        total_size += size
        min_size = min(min_size, size)
        max_size = max(max_size, size)
    
    avg_size = total_size / len(response_files) if response_files else 0
    
    print(f"   - å¹³å‡å¤§å°: {avg_size/1024:.1f} KB")
    print(f"   - æœ€å°/æœ€å¤§: {min_size/1024:.1f} KB / {max_size/1024:.1f} KB")
    
    # æ£€æŸ¥æœ€è¿‘çš„å“åº”
    latest_file = max(response_files, key=os.path.getctime)
    print(f"   - æœ€æ–°æ–‡ä»¶: {os.path.basename(latest_file)}")

def create_summary_report(all_results, output_dir):
    """åˆ›å»ºæ±‡æ€»æŠ¥å‘Š"""
    report_path = os.path.join(output_dir, "summary_report.txt")
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("PDF Research Paper Analysis Summary Report\n")
        f.write("=" * 60 + "\n")
        f.write(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"Total Documents: {len(all_results)}\n\n")
        
        # ä¸ºæ¯ä¸ªé—®é¢˜åˆ›å»ºæ‘˜è¦
        for question_id in QUESTION_IDS:
            f.write(f"\n{question_id.replace('_', ' ').title()}:\n")
            f.write("-" * 40 + "\n")
            
            # æ”¶é›†è¯¥é—®é¢˜çš„æ‰€æœ‰ç­”æ¡ˆ
            answers = []
            for doc_name, results in all_results.items():
                if question_id in results:
                    answer = results[question_id].split('\nSource:')[0].strip()
                    if answer and "[è§£æå¤±è´¥]" not in answer and "[æŸ¥è¯¢å¤±è´¥]" not in answer and answer != "N/A":
                        answers.append(f"â€¢ {doc_name}: {answer[:100]}{'...' if len(answer) > 100 else ''}")
            
            if answers:
                for answer in answers[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªç­”æ¡ˆ
                    f.write(f"{answer}\n")
                if len(answers) > 5:
                    f.write(f"... and {len(answers) - 5} more\n")
            else:
                f.write("No valid answers found\n")
    
    print(f"ğŸ“„ æ±‡æ€»æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")
    return report_path