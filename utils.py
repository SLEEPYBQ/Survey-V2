import os
import csv
import json
import glob
from datetime import datetime
from config import QUESTION_IDS
import pandas as pd

def save_results_to_xlsx(all_results, output_dir):
    """ä¿å­˜ç»“æœåˆ°XLSXæ–‡ä»¶ï¼Œå°†ç­”æ¡ˆå’Œæºä¿¡æ¯åˆ†ä¸ºä¸¤è¡Œ"""
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    # æ„å»ºè¡¨å¤´ï¼šæ·»åŠ ç±»å‹æ ‡è¯†ç¬¦åˆ—
    fieldnames = ['document', 'content_type'] + QUESTION_IDS
    
    # ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„æ–‡ä»¶å
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    xlsx_filename = f"query_results_{timestamp}.xlsx"
    xlsx_path = os.path.join(output_dir, xlsx_filename)
    
    # åŒæ—¶åˆ›å»ºä¸€ä¸ªæœ€æ–°ç»“æœçš„é“¾æ¥ï¼ˆè¦†ç›–ï¼‰
    latest_xlsx_path = os.path.join(output_dir, "query_results_latest.xlsx")
    
    # ç»Ÿè®¡ä¿¡æ¯
    stats = {
        'total_documents': len(all_results),
        'parse_failures': 0,
        'query_failures': 0,
        'empty_answers': 0
    }
    
    # æ„å»ºDataFrame - æ¯ä¸ªæ–‡æ¡£äº§ç”Ÿä¸¤è¡Œï¼šä¸€è¡Œç­”æ¡ˆï¼Œä¸€è¡Œæºä¿¡æ¯
    rows = []
    for doc_name, results in all_results.items():
        # åˆ†ç¦»ç­”æ¡ˆå’Œæºä¿¡æ¯
        answer_row = {'document': doc_name, 'content_type': 'answer'}
        source_row = {'document': doc_name, 'content_type': 'source'}
        
        for question_id in QUESTION_IDS:
            if question_id in results:
                full_content = results[question_id]
                
                # ç»Ÿè®¡ä¿¡æ¯
                if "[è§£æå¤±è´¥]" in full_content:
                    stats['parse_failures'] += 1
                elif "[æŸ¥è¯¢å¤±è´¥]" in full_content:
                    stats['query_failures'] += 1
                
                # åˆ†ç¦»ç­”æ¡ˆå’Œæºä¿¡æ¯
                if "\nSource:" in full_content:
                    answer_part, source_part = full_content.split("\nSource:", 1)
                    answer_row[question_id] = answer_part.strip()
                    source_row[question_id] = source_part.strip()
                    
                    # æ£€æŸ¥æ˜¯å¦ä¸ºç©ºç­”æ¡ˆ
                    if answer_part.strip() in ["N/A", ""]:
                        stats['empty_answers'] += 1
                else:
                    # å¦‚æœæ²¡æœ‰Sourceæ ‡è®°ï¼Œæ•´ä¸ªå†…å®¹ä½œä¸ºç­”æ¡ˆ
                    answer_row[question_id] = full_content.strip()
                    source_row[question_id] = ""
                    
                    if full_content.strip() in ["N/A", ""]:
                        stats['empty_answers'] += 1
            else:
                answer_row[question_id] = ""
                source_row[question_id] = ""
        
        # æ·»åŠ ç­”æ¡ˆè¡Œå’Œæºä¿¡æ¯è¡Œ
        rows.append(answer_row)
        rows.append(source_row)
    
    # åˆ›å»ºDataFrame
    df = pd.DataFrame(rows, columns=fieldnames)
    
    # ä¿å­˜ä¸ºxlsx
    df.to_excel(xlsx_path, index=False)
    # å¤åˆ¶åˆ°æœ€æ–°ç»“æœæ–‡ä»¶
    df.to_excel(latest_xlsx_path, index=False)
    
    # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
    stats_path = os.path.join(output_dir, f"query_stats_{timestamp}.json")
    with open(stats_path, 'w', encoding='utf-8') as f:
        json.dump(stats, f, indent=2)
    
    print(f"\nğŸ’¾ æŸ¥è¯¢ç»“æœå·²ä¿å­˜åˆ°:")
    print(f"   - ä¸»æ–‡ä»¶: {xlsx_path}")
    print(f"   - æœ€æ–°ç»“æœ: {latest_xlsx_path}")
    print(f"   - ç»Ÿè®¡ä¿¡æ¯: {stats_path}")
    
    # æ˜¾ç¤ºç»Ÿè®¡æ‘˜è¦
    if stats['total_documents'] > 0:
        total_questions = stats['total_documents'] * len(QUESTION_IDS)
        successful_parses = total_questions - stats['parse_failures'] - stats['query_failures']
        print(f"\nğŸ“Š æ•°æ®è´¨é‡ç»Ÿè®¡:")
        print(f"   - æˆåŠŸè§£æ: {successful_parses}/{total_questions} ({successful_parses/total_questions*100:.1f}%)")
        print(f"   - ç©ºå€¼/N/A: {stats['empty_answers']} ({stats['empty_answers']/total_questions*100:.1f}%)")
        print(f"   - è§£æå¤±è´¥: {stats['parse_failures']} ({stats['parse_failures']/total_questions*100:.1f}%)")
        print(f"   - æ€»è¡Œæ•°: {len(rows)} (æ¯ä¸ªæ–‡æ¡£2è¡Œï¼šç­”æ¡ˆè¡Œ+æºä¿¡æ¯è¡Œ)")
    
    return xlsx_path

def validate_raw_responses(raw_response_dir='raw_responses'):
    """éªŒè¯åŸå§‹å“åº”ç›®å½•ä¸­çš„æ–‡ä»¶"""
    if not os.path.exists(raw_response_dir):
        print(f"âš ï¸  åŸå§‹å“åº”ç›®å½•ä¸å­˜åœ¨: {raw_response_dir}")
        return
    
    response_files = glob.glob(os.path.join(raw_response_dir, "*.txt"))
    
    if not response_files:
        print(f"âš ï¸  åŸå§‹å“åº”ç›®å½•ä¸ºç©º: {raw_response_dir}")
        return
    
    print(f"\nğŸ“ åŸå§‹å“åº”éªŒè¯:")
    print(f"   - æ‰¾åˆ° {len(response_files)} ä¸ªå“åº”æ–‡ä»¶")
    
    # æ£€æŸ¥æ–‡ä»¶å¤§å°
    total_size = 0
    min_size = float('inf')
    max_size = 0
    
    for file_path in response_files:
        size = os.path.getsize(file_path)
        total_size += size
        min_size = min(min_size, size)
        max_size = max(max_size, size)
    
    avg_size = total_size / len(response_files) if response_files else 0
    
    print(f"   - å¹³å‡å¤§å°: {avg_size/1024:.1f} KB")
    print(f"   - æœ€å°/æœ€å¤§: {min_size/1024:.1f} KB / {max_size/1024:.1f} KB")
    
    # æ£€æŸ¥æœ€è¿‘çš„å“åº”
    latest_file = max(response_files, key=os.path.getctime)
    print(f"   - æœ€æ–°æ–‡ä»¶: {os.path.basename(latest_file)}")

def create_summary_report(all_results, output_dir):
    """åˆ›å»ºæ±‡æ€»æŠ¥å‘Š"""
    report_path = os.path.join(output_dir, "summary_report.txt")
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("PDF Research Paper Analysis Summary Report\n")
        f.write("=" * 60 + "\n")
        f.write(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"Total Documents: {len(all_results)}\n\n")
        
        # ä¸ºæ¯ä¸ªé—®é¢˜åˆ›å»ºæ‘˜è¦
        for question_id in QUESTION_IDS:
            f.write(f"\n{question_id.replace('_', ' ').title()}:\n")
            f.write("-" * 40 + "\n")
            
            # æ”¶é›†è¯¥é—®é¢˜çš„æ‰€æœ‰ç­”æ¡ˆ
            answers = []
            for doc_name, results in all_results.items():
                if question_id in results:
                    full_content = results[question_id]
                    # æå–ç­”æ¡ˆéƒ¨åˆ†ï¼ˆSourceä¹‹å‰çš„å†…å®¹ï¼‰
                    if "\nSource:" in full_content:
                        answer = full_content.split('\nSource:')[0].strip()
                    else:
                        answer = full_content.strip()
                    
                    if answer and "[è§£æå¤±è´¥]" not in answer and "[æŸ¥è¯¢å¤±è´¥]" not in answer and answer != "N/A":
                        answers.append(f"â€¢ {doc_name}: {answer[:100]}{'...' if len(answer) > 100 else ''}")
            
            if answers:
                for answer in answers[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªç­”æ¡ˆ
                    f.write(f"{answer}\n")
                if len(answers) > 5:
                    f.write(f"... and {len(answers) - 5} more\n")
            else:
                f.write("No valid answers found\n")
    
    print(f"ğŸ“„ æ±‡æ€»æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")
    return report_path